在学习DSSM的时候，很容易和孪生网络搞混，我这里稍微总结一下自己的思考；



对于孪生网络，并不是一个网络的名称，而是一种网络的名称；

它的特点就是encoder参数共享，也就是在对句子或者图像编码的网络权重共享；

它的网络的输入形式是这样的:：(X1,X2,Y)；X1，X2是两个输入文本，Y是我们的标签数据；

对于孪生网络来说，一般的损失函数是对比损失函数：Contrastive Loss

什么是对比损失函数呢？公式如下：

![image-20210114154303377](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-05-19-065215.png)

这个公式需要注意两个细节点，一个是d，代表的是距离度量，一个是margin，代表的是一个超参；

那么我感兴趣的是孪生网路可以不可以使用其他的损失函数？如果可以，又是哪些呢？

首先当然是可以，只要能够优化都可以；

![image-20210114154713043](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-05-19-065221.png)

参考自这里：

Siamese network 孪生神经网络--一个简单神奇的结构 - mountain blue的文章 - 知乎 https://zhuanlan.zhihu.com/p/35040994

# 三种损失函数形式

我把自己思考的三种损失函数形式总结在这里，之后有问题再回来修改:

两个向量做consine相似度或者欧氏距离度量函数，然后归一化到0-1，然后做二分类交叉熵损失函数；

两个向量做cosine相似度或者欧氏距离，带入到对应的对比损失函数

两个向量做拼接或者其他操作，然后接单个或者多个全连接，然后可以做逻辑回归或者做softmax；

# fassi做向量召回-样本重复和consine度量疑惑点

我其实一直有一个疑问，在做向量召回的时候，一般的操作就是双塔模型，然后存储对应的样本的向量，存储到fassi中，然后搜索的时候使用找最近的向量就够了；

这里面我最开始理解的时候有两个疑惑点，一个是如果做到同一个样本不会有多个向量；

我的误解原因是没有对向量的是如何落地有很好的理解；我开始的理解是模型训练好了之后，进入一个pair（query，d），分别计算向量，然后存储，这样当然会出现同一个d，出现在不同的pair；

但是，由于之间没有交互，右边这个塔模型参数不变，得到的向量当然也不变，也就是同一个d不会出现多个向量；

而且在计算的时候，不用输入一个pair对，只需要对右边这个单塔输入去重之后的d就可以；

第二个问题，为什么在faiss中使用最近的向量可以（先不用计较度量方式）得到相似的向量，而在模型中，我们在得到cosine或者其他度量结果之后，还会再接一个sigmoid；

优化sigmoid的输出的时候，cosine的值或者其他度量的值也会同等趋势变化，所以可以起到作用；

然后多说一下，如果用到向量召回，中间还是需要一个度量的值的，需要和faiss对应上，如果接一个MLP，感觉就够呛。

