BERT模型的压缩大致可以分为：1. 参数剪枝；2. 知识蒸馏；3. 参数共享；4. 低秩分解。

其中，对于剪枝，比较简单，但是容易误操作降低精读；

对于知识蒸馏，之前我写个一系列的文章，重点可以看一下这里：

对于参数共享和低秩分解，就和今天分享的[ALBERT](https://arxiv.org/pdf/1909.11942.pdf, "ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS")息息相关；

它减少了BERT的参数，但是需要注意的一个细节点是，同等规格下，ALBERT速度确实变快，但是并不明显（和大量自媒体文章解读给大家的印象差距很大）；

举个形象的例子就是，（这个例子并不严谨，只是帮助理解）参数共享让它训练的时候把多层压缩为一层去训练，但是在预测的时候，我们需要再展开多层去进行预测。

主要掌握以下的几个知识点：

1. 词向量嵌入参数分解
2. 跨层参数分享
3. 取消NSP，使用SOP
4. 预训练的时候采用更满的数据/n-gram mask方式

# 1.词向量嵌入分解

词向量嵌入参数分解，简单说就是将词向量矩阵分解为了两个小矩阵，将隐藏层的大小和词汇矩阵的大小分离开。

在Bert中，词汇表embedding大小是$V*H$；

Albert 的参数分解是这样的，将这个矩阵分解为两个小矩阵：$V*E$和$E*H$

这样做有什么好处呢？

如果说，我觉得我的模型表达能力不够，我想要通过增大隐层H的大小来提升我们模型能力的表达能力，那么在提升H的时候，不仅仅隐层参数增多，词汇表的embedding矩阵维度也在增多，参数量也在增大。

矩阵分解之后，我们可以只是做到提升隐层大小，而不去改变表词汇表的大小。

# 2.跨层参数分享

跨层参数分享，这个操作可以防止参数随着网络层数的增大而增加。

![跨层参数共享](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063530.jpg)

分为三种形式，只是共享attentions，只是共享FFN，全部共享。

共享的意思就是我这部分结构只使用同样的参数，在训练的时候只需要训练这一部分的参数就可以了。

看表格我们可以发现一个细节，就是只是共享FFN比只是共享attention的参数，模型效果要降低的多。

小声嘀咕一下，这是不是说明FFN比attention在信息表达上要重要啊。或者说attention在学习信息表达的时候。attention层学习共性比较多。FFN学习到的差异性比较多。这只是我自己的猜测哈。

# 3. SOP

作者认为，NSP不必要。与MLM相比，NSP失效的主要原因是其缺乏任务难度。

NSP样本如下:

- 从训练语料库中取出两个连续的段落作为正样本
- 从不同的文档中随机创建一对段落作为负样本

NSP将主题预测和连贯性预测合并为一个单项任务；


但是，与连贯性预测相比，主题预测更容易学习，并且与使用MLM损失学习的内容相比，重叠性更大。


对于ALBERT，作者使用了句子顺序预测（SOP）损失，它避免了主题预测，而是着重于句间建模。

其实就是预测句子顺序，正样本是顺着，负样本是颠倒过来。都是来自同一个文档。

![SOP](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063528.jpg)



# 其他细节

1. 数据格式：**Segments-Pair**

这个在RoBERTa中也有谈到，更长的序列长度可以提升性能。

2. Masked-ngram-LM

![Masked-ngram-LM](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063529.jpg)

这就有点类似百度的ERINE和SpanBERT了

3. 推测速度

![ALBERT推理速度](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-064022.png)

从图中知道，同一规模ALBERT和BERT，比如同为Base：

BERT base: 4.7x；ALBERT base：5.6x；**速度确实变快，但是确实加速并不明显**；

同等效果的情况下，比如BERT base（Avg=82.3）和ALBERT large（Avg=82.4）：

BERT base：4.7x；ALBERT large：1.7x；速度变慢了

# 总结

总结一下可以学习的思路：

1. 预训练的时候，数据填充的更满，到512这种，有利于提升模型效果，这点在RoBERTa有谈到
2. mask n-gram有利于提升效果，这点类似百度的ERINE和SpanBERT了
3. 词向量矩阵分解能减少参数，但是也会降低性能
4. 跨层参数分享可以降低参数，也会降低性能，通过实验图知道，attention共享效果还好，FFN共享效果降低有点多
5. 取消NSP，使用SOP，正负样本来自同一个文档，但是顺序不同。
6. **推理速度来看，同等规格，ALBERT速度确实变快，但是并不明显，同等效果，速度变慢**；https://kexue.fm/archives/7846)