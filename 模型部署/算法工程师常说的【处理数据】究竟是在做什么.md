先例行絮叨一下：

因为我自己平时会去专门阅读大量的论文和文章，很多大佬的文章写得真的让人赞叹；

之后我会把这些**质量不错的文章分享**给大家，**质量肯定有保障**，大家放心；

**原创还是会有的**，也一定保证质量；

接下里，我以**NLP算法工程师为例，讲一下工作中处理数据的大致过程**；

# 1.数据的存储-HDFS/Hive

首先第一步，讲一下数据的存储和更新；

对于一个互联网公司来讲，因为数据量级比较大，数据的存储一般是放在HDFS/Hive中；

对于这些数据，时间分区可能是每天/每周/每月；更新量级一般是增量/全量更新；

举个例子，比如有一个Hive表格存储的是全部的视频以及对应信息；

如果对存储信息的要求不是那么高，那么第二天更新的时候只需要增量更新第一天增加或者删除的对应的视频信息（包括视频标题，作者，时间等等）；

如果说对存储信息的要求很高，那么第二天更新的时候需要全量更新表格；比如说之前的存在的某些视频的信息发生了变化，于是需要全量更新一次，保证信息的准确；

（以上只是一个简单的例子，帮助大家理解，实际存储与更新肯定考虑的比这个周全的多）；

对于这些数据的存储与更新，一般有专门的大数据开发人员去处理，算法工程师不需要介入这一块；

但是算法工程师需要使用一些基本的命令去查看对应的Hive表格信息，比如分区信息，列表信息，确保可以快速而准确的获取对应的数据；

# 2. 原始数据的获取与处理

没有数据，模型就【巧妇难为无米之炊】；

所以数据的获取是必备的一个过程；

大致流程是这样的，比如接到一个需求，算法工程是需要能够识别出自己需要哪些数据，比如评论的文本信息，评论的时间，评论的用户等等；

在公司中，所有数据不可能全部存储在一个数据表中；

可能用户信息存储在A，视频数据存储在B，评论数据存储在C等等；

那么算法工程师需要熟练的使用Hive Sql或者Spark/PySpark等大数据工具，从数据表格中准确的取到对应的数据；

同时也需要对数据进行处理；

NLP这边一般涉及到的是对文本的处理，也就是大家常听的清洗数据；

这一块的代码大家最后平时维护一个简单的代码库，到时候直接使用就可以；

当然，如果处理数据的时候，数据量比较少，直接本地处理就可以，这种情况一般不多；

如果数据量比较大，我这边一般使用的是Hadoop streaming或者Spark在集群上处理就可以；

还需要注意的一点是，在优化整个模型的时候，我们会仔细的去看一些badcase，会有很多case是可能是由于数据清洗的不干净，这个时候我们会重新清洗数据；

所以说清洗数据不是一次就完成的，而是多次优化；

# 3. 数据的标注与处理

如果是一个有监督模型，那么就需要标注数据；

对于标注数据这块，可以大致分为【正规军】和【游击队】（我自己取得名字，理解就好hhh）；

对于【正规军】，就是对于一个互联网公司，会开发自己的标注后台。

那么算法工程师要做的就是获取原始数据，然后和开发人员对接需求，把原始数据对接到标注后台，给到专门的标注人员去标注；

大致流程是这样，首先算法工程师会根据规则（比如用户，时间，地区等限制）圈定一批数据（比如近三个月某些视频的评论数据）。

取到数据之后，和对应的开发人员&标注人员沟通需求，一般来说就是告知需要对标注有哪些要求；

举个最简单的例子，如果是一个情感分析的例子，需要标注出句子情感程度，是否包含可以人为识别出来的情感词等等；

这个步骤算法人员需要考虑周全，避免之后标注有问题导致的重新标注（如果出现这种情况，标注人员估计会疯了）；

但是，对于某些组来说，并没有对应的标注人员或者说没有权限使用公司使用的标注后台，这个时候就需要【游击队】。

一般可以这样去做，首先就是联合运营产品全组同学，进行人工标注；虽然这个方法比较low（没有贬义哈），但也算是一种方法；

这种情况的问题就是运营同学肯定有自己平时的工作，而且可能对于标注流程和规则并不熟悉，会出现比较多的标注错误；

其次就是算法工程会尝试一下无监督方法或者半监督增强数据，或者使用BERT进行冷启动；

对于BERT冷启动这块，大家可能深入研究一下，有很多细节可以挖。

整个数据处理的流程大概就是这样，就是取数据，标注数据，清洗数据，看badcase，优化清洗方法；

枯燥生活，朴实无华；